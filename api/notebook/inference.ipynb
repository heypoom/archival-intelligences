{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.getlogin()\n",
    "os.path.expanduser(\"~\")\n",
    "\n",
    "from requests import get\n",
    "\n",
    "ip = get('https://api.ipify.org').content.decode('utf8')\n",
    "print('My public IP address is: {}'.format(ip))"
   ],
   "id": "1be1f2d249b4c1c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from diffusers import DPMSolverMultistepScheduler, DiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
    "\n",
    "scheduler = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    scheduler=scheduler,\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_encoder,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"mps\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba82c56bac04200b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = pipeline(\"chua mia tee painting, tree\", num_inference_steps=20).images[0]\n",
    "image"
   ],
   "id": "b5da68582409b096",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "cpipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "cpipe.load_lora_weights(\n",
    "    \"heypoom/chuamiatee-1\",\n",
    "    weight_name=\"pytorch_lora_weights.safetensors\"\n",
    ")"
   ],
   "id": "252ad8d439c7b936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = cpipe(\"chua mia tee painting, flags\", num_inference_steps=100).images[0]\n",
    "image"
   ],
   "id": "5bc8a427e0560e7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "image.save(\"./chua_mia_tee.png\")",
   "id": "7b5919b5f1fce651",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "denoised_images = []\n",
    "\n",
    "def latents_to_rgb(latents):\n",
    "    weights = (\n",
    "        (60, -60, 25, -70),\n",
    "        (60,  -5, 15, -50),\n",
    "        (60,  10, -5, -35)\n",
    "    )\n",
    "\n",
    "    weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))\n",
    "    biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)\n",
    "    rgb_tensor = torch.einsum(\"...lxy,lr -> ...rxy\", latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(-1)\n",
    "    image_array = rgb_tensor.clamp(0, 255)[0].byte().cpu().numpy()\n",
    "    image_array = image_array.transpose(1, 2, 0)\n",
    "\n",
    "    return Image.fromarray(image_array)\n",
    "\n",
    "def denoising_callback(pipe, step, timestep, callback_kwargs):\n",
    "    latents = callback_kwargs[\"latents\"]\n",
    "    image = latents_to_rgb(latents)\n",
    "    denoised_images.append(image)\n",
    "\n",
    "    return callback_kwargs\n",
    "    \n",
    "prompt = \"a rabbit sleeps\"\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    prompt,\n",
    "    num_inference_steps=5,\n",
    "    callback_on_step_end=denoising_callback,\n",
    "    callback_on_step_end_tensor_inputs=['latents'],\n",
    "    guidance_scale=7.5\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a3003384d3e9da4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig, axs = plt.subplots(1, len(denoised_images), figsize=(20, 4))\n",
    "\n",
    "for i in range(len(denoised_images)):\n",
    "    axs[i].imshow(denoised_images[i])\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e0dd44b7b97bdbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from diffusers import StableDiffusionImageVariationPipeline\n",
    "# \n",
    "# variation_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n",
    "#   \"lambdalabs/sd-image-variations-diffusers\",\n",
    "#   revision=\"v2.0\",\n",
    "#   torch_dtype=torch.float16,\n",
    "# ).to(\"mps\")\n",
    "# \n",
    "# v_out = variation_pipe(malaya, num_images_per_prompt=5, num_inference_steps=50, guidance_scale=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f03d5d14ef41bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "malaya = Image.open(\"./malaya.png\")",
   "metadata": {
    "collapsed": false
   },
   "id": "1a1b4392cd64192a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\"\n",
    ").to(\"mps\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ecb7742c7ea39a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[g + 0.5 for g in range(0, 6)]",
   "id": "3424f0c87f984e1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "[i / 5 for i in range(0, 6)]",
   "id": "38cb4cdef47651c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "out_images = []\n",
    "\n",
    "Gc = 2\n",
    "Sc = 6\n",
    "SIZE = 512\n",
    "\n",
    "for g in range(0, Gc):\n",
    "    for s in range(0, Sc):\n",
    "        result = img_pipe(\n",
    "            prompt=\"a dream\",\n",
    "            image=malaya.resize((SIZE, SIZE)).convert(\"RGB\"),\n",
    "            strength=s / 5,\n",
    "            guidance_scale=g + 5.5,\n",
    "            num_images_per_prompt=1,\n",
    "            num_inference_steps=20,\n",
    "        )\n",
    "        \n",
    "        out_images.append(result.images[0])"
   ],
   "id": "91b660cfc4bf5761",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))  # width, height in inches\n",
    "\n",
    "for i in range(Gc):\n",
    "    for j in range(Sc):\n",
    "        index = i * Sc + j\n",
    "        ax = plt.subplot(Gc, Sc, index + 1)  # nrows, ncols, index\n",
    "        plt.imshow(out_images[index])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "da3dca11b8e279f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
